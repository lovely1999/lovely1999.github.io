{"meta":{"title":"朽月","subtitle":null,"description":null,"author":"John Doe","url":"http://wanquanxiaobai.com","root":"/"},"pages":[{"title":"404 Not Found","date":"2019-04-14T12:03:36.226Z","updated":"2019-04-14T12:03:36.226Z","comments":true,"path":"404.html","permalink":"http://wanquanxiaobai.com/404.html","excerpt":"","text":"404 Not Found 很抱歉，您访问的页面不存在可能是输入地址有误或该地址已被删除"},{"title":"所有分类","date":"2019-04-14T12:00:49.024Z","updated":"2019-04-14T12:00:49.024Z","comments":true,"path":"categories/index.html","permalink":"http://wanquanxiaobai.com/categories/index.html","excerpt":"","text":""},{"title":"我的朋友们","date":"2019-04-14T12:02:58.881Z","updated":"2019-04-14T12:02:58.881Z","comments":true,"path":"friends/index.html","permalink":"http://wanquanxiaobai.com/friends/index.html","excerpt":"","text":""},{"title":"","date":"2019-04-14T12:01:57.122Z","updated":"2019-04-14T12:01:57.122Z","comments":true,"path":"mylist/index.html","permalink":"http://wanquanxiaobai.com/mylist/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2019-04-14T12:00:47.481Z","updated":"2019-04-14T12:00:47.481Z","comments":true,"path":"tags/index.html","permalink":"http://wanquanxiaobai.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"爬虫学习笔记--03","slug":"Python03","date":"2019-05-05T04:23:46.000Z","updated":"2019-05-05T06:50:34.424Z","comments":true,"path":"2019/05/05/Python03/","link":"","permalink":"http://wanquanxiaobai.com/2019/05/05/Python03/","excerpt":"想在网页上找到一些有用的信息该怎么办呢？承接上文，一起来看看吧！！！","text":"想在网页上找到一些有用的信息该怎么办呢？承接上文，一起来看看吧！！！ Python爬虫网页下载器 网页下载器：将互联网上URL对应的网页下载到本地的工具 Python有几种网页下载器 urllib2（Python官方基础模块） requests(第三方插件) urllib2下载网页的方法 最简洁的方法 (`) import urllib2 #直接请求response = urllib2. (`)","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://wanquanxiaobai.com/categories/爬虫/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://wanquanxiaobai.com/tags/Python/"}]},{"title":"爬虫学习笔记--02","slug":"Python02","date":"2019-05-02T04:11:46.000Z","updated":"2019-05-05T04:22:37.354Z","comments":true,"path":"2019/05/02/Python02/","link":"","permalink":"http://wanquanxiaobai.com/2019/05/02/Python02/","excerpt":"想在网页上找到一些有用的信息该怎么办呢？承接上文，一起来看看吧！！！","text":"想在网页上找到一些有用的信息该怎么办呢？承接上文，一起来看看吧！！！ URL管理器 概念：管理待抓取URL集合和已抓取URL集合 作用： 防止重复抓取、防止循环抓取 需要包含的功能 添加新的URL到待爬取的集合中 判断待添加的URL是否在容器中 判断是否还有待爬取的URL 获取待爬取的URL 将URL从待爬取移动到已爬取 实现方式 将URL存放到内存中 在python中可以将待爬取和已爬取的URL集合放置于 set() 中python中set()可以直接去除集合中重复的元素 将URL存储于关系型数据库中 如果在MySQL中即可建立一表，其结构如下urls(url,is_crawled)is_crawled字段来标志此URL是否已被爬取 可以将URL存储于缓冲数据库中 如readis，它支持set数据结构可以将以将待爬取和已爬取的URL集合放置于 set 中优点：高性能","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://wanquanxiaobai.com/categories/爬虫/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://wanquanxiaobai.com/tags/Python/"}]},{"title":"爬虫学习笔记--01","slug":"Python01","date":"2019-05-01T14:26:38.000Z","updated":"2019-05-05T03:05:22.785Z","comments":true,"path":"2019/05/01/Python01/","link":"","permalink":"http://wanquanxiaobai.com/2019/05/01/Python01/","excerpt":"想在网页上找到一些有用的信息该怎么办呢？","text":"想在网页上找到一些有用的信息该怎么办呢？ 爬虫准备工作 参考资料 Python网络数据采集 · 图灵工业出版 精通Python爬虫框架Scrapy · 人民邮电出版社 前提知识 URL http协议 web前端 · HTML、css、js ajax re、xpath xml 爬虫简介 爬虫定义： 网络爬虫，即Web Spider，是一个很形象的名字。 把互联网比喻成一个蜘蛛网，那么Spider就是在网上爬来爬去的蜘蛛。 网络蜘蛛是通过网页的链接地址来寻找网页的。 从网站某一个页面（通常是首页）开始，读取网页的内容，找到在网页中的其它链接地址， 然后通过这些链接地址寻找下一个网页，这样一直循环下去，直到把这个网站所有的网页都抓取完为止。 如果把整个互联网当成一个请叫我汪海网站，那么网络蜘蛛就可以用这个原理把互联网上所有的网页都抓取下来。 这样看来，网络爬虫就是一个爬行程序，一个抓取网页的程序。 两大特征 能按照作者的要求下载数据或内容 能自动在网络上去流窜 三大步骤： 下载信息（网页） 提取正确的信息 根据一定的规则自动跳转到另外的网页上执行上两步的内容 爬虫的分类： 通用爬虫 专用爬虫（聚焦爬虫） Python网络包简介 Python2.x：urllib /urllib2 / urllib3 / httplib / httplib2 / requests Python3.x：urllib /urllib2 / urllib3 / requests 爬虫的价值 互联网数据为我所用！！","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://wanquanxiaobai.com/categories/爬虫/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://wanquanxiaobai.com/tags/Python/"}]},{"title":"关于本站的起因","slug":"pag01","date":"2019-05-01T08:34:55.000Z","updated":"2019-05-01T08:52:31.147Z","comments":true,"path":"2019/05/01/pag01/","link":"","permalink":"http://wanquanxiaobai.com/2019/05/01/pag01/","excerpt":"每件事总有它的起因和结果","text":"每件事总有它的起因和结果 关于博客的前因后果在学习了编程2年后的一天，突然想到为什么我不自己记录一下自己的学习经历呢？这样一方面可以让自己学习更有动力；另一方面，也可以让自己的疑问和得到的答案有地方记录，方便自己以后查看。所以我在网上到处找关于记录编程笔记之类的方法。最后选择了使用个人博客这一方式来记录我的学习。 以此共勉在找不着方向的时候，不要放弃。生活总是光明，人生总是精彩。不必为了一次小的挫折就垂头丧气，也不必为了一次小的成功就沾沾自喜。","categories":[{"name":"随笔","slug":"随笔","permalink":"http://wanquanxiaobai.com/categories/随笔/"}],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2019-03-17T00:56:15.630Z","updated":"2019-05-01T05:56:53.317Z","comments":true,"path":"2019/03/17/hello-world/","link":"","permalink":"http://wanquanxiaobai.com/2019/03/17/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}